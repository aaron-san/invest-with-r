---
title: Forecasting S&P 500 Returns with R (Part 1)
author: Aaron Hardy
date: '2020-06-11'
slug: forecasting-s-p-500-returns-with-r-part-1
categories: []
tags:
  - Stocks
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
editor_options: 
  chunk_output_type: console
---



<div id="introduction" class="section level2">
<h2>1. Introduction</h2>
<p>Is it possible to predict stock returns? We will look at a group of 500 large U.S. companies and try to identify any historical patterns that can be used to predict future returns. Specifically, we will focus on the S&amp;P 500 stock index which represents the stock performance of 500 of the largest U.S. companies.</p>
<p>Why predict S&amp;P 500 returns? Millions of people are invested in U.S. stocks through pension plans and individual investment accounts. Many of these investors want to protect their investments by buying before prices rise and selling before prices fall.</p>
<p>We will conduct analysis in three broad steps:</p>
<ul>
<li>Data preparation,</li>
<li>Model fitting, and</li>
<li>Evaluation of model performance</li>
</ul>
<div id="data" class="section level3">
<h3>Data</h3>
<ul>
<li>S&amp;P 500 monthly returns (FRED symbol: <em>SP500</em>)</li>
</ul>
<p>This data is available on the <a href="https://fred.stlouisfed.org">FRED</a> website.</p>
</div>
</div>
<div id="initial-setup---load-the-packages-and-dataset" class="section level2">
<h2>2. Initial Setup - Load the packages and dataset</h2>
<pre class="r"><code># Load libraries
suppressPackageStartupMessages({
    
    # Multipurpose
    library(tidyverse) # for mutate(), select(), filter()
    library(tidyquant) # for ROC(), endpoints()
    library(data.table) # for fread()
    library(readODS) # for read_ods()
    library(lubridate) # for days()
    library(magrittr) # for is_less_than()
    library(janitor) # for clean_names()
    
    # Models
    library(tidymodels)
    library(feasts) # for STL()
    library(lmtest) # bptest() for heteroskedasticity
    library(forecast) # forecast() time series
    
    # Time series
    library(tsibble) # tsibble for time series based on tidy principles
    library(fable)  # for forecasting based on tidy principles
    library(timetk) # tk_ts, tk_tbl
    
    # Plotting
    library(ggthemes)
    
    # Data processing
    library(sweep) # for sw_tidy(), sw_glance(), sw_augment()
    library(furrr) # for future_map() parallel version of map()

})

# Variables
symbols &lt;- &quot;SP500&quot;

# Download data
# econ_data &lt;- symbols %&gt;%
#   tq_get(get = &quot;economic.data&quot;, 
#          from = &quot;1975-12-31&quot;, to = &quot;2020-05-19&quot;)


# Load data
tickers_file &lt;- list.files(&quot;C:/Users/AaronH/Desktop/Aaron/R/Data&quot;, pattern = &quot;FRED Tickers.ods&quot;, full.names = TRUE) %&gt;% max()
current_econ_data_file &lt;- list.files(&quot;C:/Users/AaronH/Desktop/Aaron/R/Data&quot;, pattern = &quot;Data from FRED&quot;, full.names = TRUE) %&gt;% max()

tickers &lt;- read_ods(tickers_file, range = &quot;A1:D50&quot;, col_types = NA) %&gt;% 
    as_tibble() %&gt;% 
    drop_na()

# Load data
data &lt;- fread(current_econ_data_file) %&gt;% 
    as_tibble() %&gt;%
    mutate(date = ymd(date)) %&gt;%
    dplyr::filter(symbol %in% symbols) %&gt;%
    group_by(symbol) </code></pre>
</div>
<div id="data-preparation-and-preprocessing" class="section level2">
<h2>3. Data Preparation and Preprocessing</h2>
<div id="prepare-the-data" class="section level3">
<h3>Prepare the data</h3>
<pre class="r"><code># Reformat the data
SP500_tbl &lt;- data %&gt;%
    complete(date = seq.Date(from = min(date), to = max(date), by = &quot;days&quot;)) %&gt;%
    arrange(date) %&gt;%
    fill(price) %&gt;%
    pivot_wider(names_from = symbol, values_from = price) %&gt;%
    slice(endpoints(date, on = &quot;month&quot;)) %&gt;%
    mutate(ROC.SP500 = ROC(SP500)) %&gt;%
    drop_na() %&gt;%
    filter(between(date, &quot;2011-01-31&quot;, &quot;2019-12-31&quot;))

# Coerce the data to a time series
SP500_ts &lt;- SP500_tbl %&gt;%
    tk_ts(select = ROC.SP500, start = .$date[1] %&gt;% as.yearmon(), frequency = 12)

# Coerce the data to a tsibble
SP500_tsi &lt;- as_tsibble(SP500_ts, index = date) %&gt;%
    rename(ROC.SP500 = value, date = index)</code></pre>
</div>
<div id="univarite-analysis" class="section level3">
<h3>Univarite Analysis</h3>
<p>Let’s look at the S&amp;P 500 time series as a univariate series. Is it possible to predict with reasonable accuracy future returns of the S&amp;P 500 just looking at historical data over the past nine years? Is there predictable seasonality, for example? Let’s take a look at the data in its original form and do some exploratory analysis.</p>
<pre class="r"><code># Scatterplot
SP500_tbl %&gt;%
    ggplot(aes(x = date, y = ROC.SP500)) + #, color = ROC.SP500 &gt; mean(ROC.SP500))) +
    geom_line(show.legend = FALSE, color = &quot;steelblue&quot;) +
    geom_hline(aes(yintercept = mean(ROC.SP500)), color = &quot;firebrick2&quot;, 
               linetype = &quot;dashed&quot;, size = 1) +
    labs(title = &quot;S&amp;P 500 monthly Returns&quot;,
         subtitle = &quot;(Mean return in red)&quot;,
         y = &quot;Return&quot;, x = &quot;&quot;) +
    scale_y_continuous(labels = scales::percent_format()) +    
    scale_x_date(date_labels = &quot;%Y %b&quot;) +
    theme_minimal()</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/scatterplot-1.png" width="672" /></p>
<pre class="r"><code>summary(SP500_tbl)</code></pre>
<pre><code>##       date                SP500        ROC.SP500        
##  Min.   :2011-01-31   Min.   :1131   Min.   :-0.096265  
##  1st Qu.:2013-04-22   1st Qu.:1590   1st Qu.:-0.005685  
##  Median :2015-07-15   Median :2061   Median : 0.012781  
##  Mean   :2015-07-16   Mean   :2063   Mean   : 0.008736  
##  3rd Qu.:2017-10-07   3rd Qu.:2510   3rd Qu.: 0.028465  
##  Max.   :2019-12-31   Max.   :3231   Max.   : 0.102307</code></pre>
<p>The simple scatter plot doesn’t seem to reveal any clear trends or seasonality. Let’s look at distributions of returns grouped by year. The violin plot is useful here.</p>
<p>The mean monthly return over the nine-year period 2011-2019 was 0.9%.</p>
<pre class="r"><code># Violin plot of returns    
SP500_tbl %&gt;%
    mutate(yr = year(date)) %&gt;%
    ggplot(aes(x = date, y = ROC.SP500)) +
    geom_violin(aes(group = yr), draw_quantiles = c(0.25, 0.5, 0.75), 
                show.legend = FALSE, fill = &quot;lightblue2&quot;, alpha = 0.5) +
    geom_hline(yintercept = 0, color = &quot;darkgrey&quot;, size = 0.7) +
    geom_hline(aes(yintercept = mean(ROC.SP500)), color = &quot;firebrick2&quot;, 
               size = 0.7, linetype = &quot;longdash&quot;) +
    labs(title = &quot;Violin plots of Returns by Year&quot;,
         subtitle = &quot;Mean in red&quot;,
         x = &quot;&quot;, y = &quot;Return&quot;) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_x_date(date_breaks = &quot;3 months&quot;, date_labels = &quot;%Y %b&quot;) +
    geom_point(aes(color = factor(yr)), size = 0.9,
               show.legend = FALSE) +
    theme_hc() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          panel.grid = element_blank()) +
    facet_grid(~ yr, scale = &quot;free&quot;)</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/violinplot-1.png" width="672" /></p>
<p>The violin plots show high volatility of returns in 2011 followed by decreased volatility in subsequent years and finally high volatility again in 2019. The range of returns (distance from bottom of violin plot to its top) seems to increase and decrease without any clear regularity. No clear patterns of volatility stand out.</p>
<p>Let’s look at some more distributions of returns, but this time grouped by month. For example, the distribution for January includes January returns for each year (2011-2019).</p>
<pre class="r"><code># Density plot of returns by month
SP500_tbl %&gt;%
    group_by(month(date)) %&gt;%
    mutate(medians = median(ROC.SP500),
           means = mean(ROC.SP500)) %&gt;%
    ggplot(aes(x = ROC.SP500, group = month(date))) +
    geom_density(aes(x = ROC.SP500, fill = factor(months(date, abbr = TRUE), levels = month.abb)), show.legend = FALSE, alpha = 0.4) +
    labs(title = &quot;Density Plot of Returns by Month&quot;,
         subtitle = &quot;(Mean in red)\n(Median in green)&quot;,
         x = &quot;Return&quot;, y = &quot;Count&quot;) +
    facet_wrap(~ factor(months(date, abbr = TRUE), levels = month.abb), ncol = 4) +
    geom_vline(xintercept = 0) +
    geom_vline(aes(xintercept = medians), color = &quot;green&quot;, 
               linetype = &quot;longdash&quot;, size = 0.8) +
    geom_vline(aes(xintercept = means), color = &quot;firebrick2&quot;, 
               linetype = &quot;longdash&quot;, size = 0.8) +

    scale_x_continuous(labels = scales::percent_format()) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90),
          panel.grid = element_blank())</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/densityplot-1.png" width="672" /></p>
<p>The density plots seem to reveal that March, May, August, November, and December experience a high proportion of returns greater than zero. This can be observed by noticing that the mean and medians of each of these months’ returns are considerably greater than the zero-lines (in black). Let’s look further. This would suggest that stocks held in these months experience consistently positive monthly returns.</p>
<p>Let’s look at a calendar plot that shows returns across months and years.</p>
<pre class="r"><code># Calendar plot
SP500_tbl %&gt;% 
    mutate(date = date - lubridate::days(1)) %&gt;%
    mutate(month = month(date),
           year = year(date)) %&gt;%
    mutate(bin = cut(ROC.SP500, breaks = c(-Inf, -0.02, 0, 0.02, Inf), labels = c(&quot;Below -2%&quot;, &quot;(-2%, 0%)&quot;, &quot;(0%, 2%)&quot;, &quot;Above 2%&quot;))) %&gt;%
    ggplot(aes(x = factor(months(date, abbr = TRUE), levels = month.abb), y = substr(year, 1, 4), fill = bin)) +
    geom_tile(color = &quot;white&quot;) +
    scale_fill_manual(values = c(&quot;firebrick2&quot;, &quot;pink&quot;, &quot;lightblue&quot;, &quot;mediumturquoise&quot;)) +
    labs(title = &quot;Calendar plot&quot;, subtitle = &quot;S&amp;P 500 Monthly Returns&quot;, 
         x = &quot;&quot;, y = &quot;&quot;, fill = &quot;&quot;) +
    theme_hc() +
    theme(panel.grid.major = element_blank(),
          panel.border= element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;,
          axis.ticks = element_blank())</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>A look at the calendar plot reveals that February, April, July, and November experience positive returns in at least seven of the nine years observed. It appears that returns in these months are consistently above zero.</p>
<pre class="r"><code># Breush Pagan Test for heteroskedasticity
# - First create a linear model and use residuals to test for heteroskedasticity
lmMod &lt;- tslm(SP500_ts ~ trend) # initial model
bptest(lmMod) %&gt;%
    sw_glance() %&gt;%
    pull(p.value) %&gt;%
    is_less_than(0.05) %&gt;%
    if_else(&quot;Heteroskedastic (alpha = 5%)&quot;, 
            &quot;Not heteroskedastic (alpha = 5%)&quot;)</code></pre>
<pre><code>## [1] &quot;Not heteroskedastic (alpha = 5%)&quot;</code></pre>
<pre class="r"><code># Apply a Box-cox transformation to stabilize volatility
# Calculate appropriate BoxCox lambda
lambda &lt;- BoxCox.lambda(SP500_ts)
SP500_BoxCox &lt;- BoxCox(SP500_ts, lambda)
SP500_BoxCox %&gt;%
    tk_tbl(rename_index = &quot;date&quot;) %&gt;%
    ggplot(aes(x = as_date(date), y = ROC.SP500, color = ROC.SP500 &gt; mean(ROC.SP500))) +
    geom_point(show.legend = FALSE, size = 2.5) +
    geom_hline(aes(yintercept = mean(ROC.SP500)), color = &quot;firebrick2&quot;, 
               linetype = &quot;dashed&quot;, size = 1) +
    labs(title = &quot;Box-Cox Transformed S&amp;P 500 Monthly Returns&quot;,
         y = &quot;&quot;, x = &quot;&quot;) +
    scale_x_date(date_labels = &quot;%Y-%b&quot;) +
    theme_minimal()</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Looking at the S&amp;P 500 monthly returns, it appears that over the nine year period, most monthly returns were positive. There also appeared to be high volatility of returns followed by low volatility and then high volatility again. However, the Breush Pagan test gave evidence that the series is not heteroskedastic (i.e., has stable volatility).</p>
<p>In addition, a BoxCox transformation was performed in an attempt to dampen the variance. The plot of the Box-Cox transformed series looked similar to the original S&amp;P 500 returns data. We decided to stay with the original data.</p>
<p>Let’s check for autocorrelation.</p>
<pre class="r"><code># Augmented Dickey-Fuller Test for stationarity
ndiffs(SP500_ts)</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code># View the ACF and PCF plots
SP500_ts %&gt;% 
    ggtsdisplay(theme = theme_minimal(), main = &quot;S&amp;P 500 returns monthly&quot;)</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># Box Test for Autocorrelation
# - Test of whether any of a group of autocorrelations of a time series are different from zero. Instead of testing randomness at each distinct lag, it tests the &quot;overall&quot; randomness based on a number of lags.
Box.test(SP500_ts, type = &quot;Ljung&quot;) %&gt;%
    sw_glance() %&gt;%
    pull(p.value) %&gt;%
    is_less_than(0.05) %&gt;%
    if_else(&quot;Autocorrelation exists (alpha = 5%).&quot;, 
            &quot;No autocorrelation exists (alpha = 5%).&quot;)</code></pre>
<pre><code>## [1] &quot;No autocorrelation exists (alpha = 5%).&quot;</code></pre>
<p>We tested if the series is trend-stationary by determining how many first differences should be applied to the series make it stationary. The result was 0, so the series is likely stationary (i.e., mean-reverting).</p>
<p>The autocorrelation function (ACF) plot shows no significant correlations with any of the prior 12 lagged months The same was observed for the partial autocorrelation function (PACF) plot. The Ljung-Box test gave evidence that no autocorrelation exists in the time series. Overall, it appears that the S&amp;P 500 monthly returns series is white noise (a.k.a., random walk). To further test that, we could take several time slices of the series and check that the means and volatilities of each subseries match those of the entire series.</p>
<p>Let’s now check for seasonality. The S&amp;P 500 returns appears to be white noise. Let’s decompose the returns and look at the seasonal component. We’ll also test if the seasonal component is useful in forecasting. We’ll check this by building two ETS models: one with a seasonal component and one without. If the RMSE of the model with the seasonal component is higher than that of the model without the seasonal component, then the seasonal component likely contributed to model performance.</p>
<pre class="r"><code># Seasonal decomposition
SP500_tsi %&gt;%
    model(STL(ROC.SP500 ~ season())) %&gt;%
    components() %&gt;%
    rename(original = ROC.SP500,
           seasonal = season_year) %&gt;%
    clean_names() %&gt;%
    dplyr::select(-model) %&gt;%
    pivot_longer(-c(date, season_adjust), names_to = &quot;component&quot;) %&gt;%
    arrange(factor(component, levels = unique(component)), date) %&gt;% 
    ggplot(aes(x = date, y = value)) +
    geom_line() +
    scale_y_continuous(labels = scales::percent_format()) +
    facet_wrap(~ component) +
    labs(title = &quot;Plot of Decomposed S&amp;P 500 Returns&quot;,
         x = &quot;&quot;, y = &quot;&quot;) +
    theme_minimal()</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/seasonal_decomposition-1.png" width="672" /></p>
<pre class="r"><code># Informal test for signifance of seasonality
# Exponential time series model with &quot;Additive&quot; Error, &quot;None&quot; trend, and &quot;Additive&quot; season types
SP500_tsi %&gt;% 
    model(ETS(ROC.SP500 ~ season(&quot;A&quot;) + trend(&quot;N&quot;) + error(&quot;A&quot;))) %&gt;% 
    accuracy() %&gt;% 
    pull(RMSE)</code></pre>
<pre><code>## [1] 0.03191992</code></pre>
<pre class="r"><code># Exponential time series model with &quot;Additive&quot; Error, &quot;None&quot; trend, and &quot;None&quot; season types
SP500_tsi %&gt;% 
    model(ETS(ROC.SP500 ~ season(&quot;N&quot;) + trend(&quot;N&quot;) + error(&quot;A&quot;))) %&gt;%
    accuracy() %&gt;% 
    pull(RMSE)</code></pre>
<pre><code>## [1] 0.03326453</code></pre>
<p>We noticed that the RMSE of the ETS model without the seasonal component was higher and therefore suggests the seasonal component did not add to the performance of the model.</p>
</div>
<div id="model---univariate-time-series" class="section level3">
<h3>Model - Univariate Time Series</h3>
<p>Although there appears to be no seasonality or significant trend(s), let’s fit several forecast models and check our prediction results. We will fit the following models:</p>
<ul>
<li><strong>Naive, random walk</strong>
<ul>
<li>This method uses the last observation as the forecast for the next period.</li>
</ul></li>
<li><strong>Naive, random walk with a drift</strong>
<ul>
<li>This method uses the last observation plus the historical trend to create a forecast for the next period.</li>
</ul></li>
<li><strong>Mean (Historical average)</strong>
<ul>
<li>This method simply uses the historical average as the forecast for the next period.</li>
</ul></li>
<li><strong>ARIMA (Autoregressive Integrated Moving Average)</strong>
<ul>
<li>This method models lags of the data as well as current and lagged errors to create forecasts.</li>
</ul></li>
<li><strong>TSLM (Time series linear model with trend and seasonality)</strong>
<ul>
<li>This method uses a linear model with trend and seasonality to create forecasts.</li>
</ul></li>
<li><strong>ETS (Exponential time series)</strong>
<ul>
<li>This method uses an exponential model with seasonality to create forecasts.</li>
</ul></li>
<li><strong>Mixed</strong>
<ul>
<li>This method simple finds an average of the forecasts created from the models above to create forecast.</li>
</ul></li>
</ul>
<pre class="r"><code># Split the time series into training and test sets
initial_time_split &lt;- initial_time_split(SP500_tsi, prop = 0.8) 
training_data &lt;- initial_time_split %&gt;% training() 
test_data &lt;- initial_time_split %&gt;% testing()

SP500_recipe &lt;- training_data %&gt;%
    recipe(ROC.SP500 ~ .) %&gt;%
        prep()

# Extract the data from the SP500_recipe object
SP500_training &lt;- SP500_recipe %&gt;% juice()

# Apply the recipe to the testing data
SP500_testing &lt;- SP500_recipe %&gt;% bake(test_data)


# Fit multiple time series models
fit &lt;- SP500_training %&gt;%
    model(
        # Naive, Random Walk Forecasts
        # Forecasts equal to last observed value (appropriate for many financial series)
        rw = RW(ROC.SP500),
        # Drift method
        # Forecasts equal to last value plus average change over series (appears as line)
        rw.drift = RW(ROC.SP500, drift = TRUE),
        # Forecasts equal to mean of historical data
        mean = MEAN(ROC.SP500),
        # Seasonal Naive
        # Forecasts equal to last value from same season
        snaive = SNAIVE(ROC.SP500 ~ lag(&quot;year&quot;)),
        # ARIMA
        # Forecasts based on lagged values of series as well as lagged errors
        arima = ARIMA(ROC.SP500),
        # TSLM (Time Series Linear Model)
        # Applies a trend, seasonal, and error terms to the data
        tslm = TSLM(ROC.SP500 ~ trend() + season()),
        # ETS (Exponential Time Series)
        # Uses an exponential model with trend and seasonality to create forecasts
        ets = ETS(ROC.SP500)) %&gt;%
        mutate(mixed = (rw + rw.drift + mean + snaive + arima + tslm + ets) / 7)

# Optimal ARIMA model parameters
fit %&gt;% select(arima) %&gt;% report()</code></pre>
<pre><code>## Series: ROC.SP500 
## Model: ARIMA(0,0,0) w/ mean 
## 
## Coefficients:
##       constant
##         0.0089
## s.e.    0.0033
## 
## sigma^2 estimated as 0.0009659:  log likelihood=177
## AIC=-350   AICc=-349.86   BIC=-345.09</code></pre>
<pre class="r"><code># Create the forecasts
fcast &lt;- fit %&gt;% 
    forecast(h = nrow(SP500_testing))

# Plot the forecasts
fcast %&gt;% 
    filter(.model %in% c(&quot;rw&quot;, &quot;snaive&quot;, &quot;rw.drift&quot;, &quot;mean&quot;)) %&gt;%
  autoplot(SP500_training) +
  labs(title = &quot;Forecasts for S&amp;P 500 monthly returns&quot;,
       x = &quot;Year&quot;, y = &quot;Monthly return&quot;) +
  guides(colour = guide_legend(title = &quot;Forecast&quot;)) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_minimal() +
    facet_wrap(~ .model)</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>fcast %&gt;% 
    filter(.model %in% c(&quot;arima&quot;, &quot;tslm&quot;, &quot;ets&quot;, &quot;mixed&quot;)) %&gt;%
  autoplot(SP500_training) +
  labs(title = &quot;Forecasts for S&amp;P 500 monthly returns&quot;,
       x = &quot;Year&quot;, y = &quot;Monthly return&quot;) +
  guides(colour = guide_legend(title = &quot;Forecast&quot;)) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_minimal() +
    facet_wrap(~ .model)</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<pre class="r"><code># Model performance
accuracy(fcast, SP500_testing) %&gt;% arrange(desc(RMSE))</code></pre>
<pre><code>## # A tibble: 8 x 9
##   .model   .type       ME   RMSE    MAE   MPE  MAPE  MASE    ACF1
##   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 rw       Test   0.0477  0.0630 0.0583 324.  338.    NaN -0.322 
## 2 rw.drift Test   0.0477  0.0630 0.0583 324.  338.    NaN -0.322 
## 3 tslm     Test  -0.00203 0.0439 0.0306  56.2  92.2   NaN -0.305 
## 4 mixed    Test   0.0123  0.0429 0.0357 126.  126.    NaN -0.288 
## 5 snaive   Test  -0.00396 0.0425 0.0293  30.2  94.1   NaN -0.0806
## 6 arima    Test  -0.00102 0.0412 0.0300  49.7  88.1   NaN -0.322 
## 7 mean     Test  -0.00102 0.0412 0.0300  49.7  88.1   NaN -0.322 
## 8 ets      Test  -0.00101 0.0412 0.0300  49.7  88.1   NaN -0.322</code></pre>
<pre class="r"><code># Check residuals
interp.residuals &lt;- fit %&gt;% 
    residuals(type=&quot;response&quot;)

# Plot residuals
interp.residuals %&gt;%
    ggplot(aes(x = date, y = .resid)) +
    geom_point() +
    geom_smooth(method = &quot;loess&quot;, se = TRUE, level = 0.95) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_x_date(date_labels = &quot;%Y&quot;) +
    facet_wrap(~ .model, ncol = 3) +
    labs(title = &quot;Residuals plot&quot;, x = &quot;&quot;, y = &quot;Residual&quot;) +
    theme_hc()</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<pre class="r"><code># Plot density of residuals
interp.residuals %&gt;%
    ggplot(aes(x = .resid)) + 
    geom_density(aes(color = .model), show.legend = FALSE) +
    scale_y_continuous(labels = scales::label_number(accuracy = 1)) +
    scale_x_continuous(labels = scales::percent_format()) +
    facet_wrap(~ .model, ncol = 3) +
    labs(title = &quot;Density plots of residuals&quot;, x = &quot;Residual&quot;, y = &quot;Frequency&quot;) +
    theme_hc()</code></pre>
<p><img src="/post/forecasting-s-p-500-returns-with-r-part-1_files/figure-html/unnamed-chunk-4-4.png" width="672" /></p>
<p>It appears that the <em>Random Walk</em> and <em>Random Walk with a Drift</em> models are equally the best models among the eight applied. They resulted in the highest RMSE for out-of-sample test data forecasting. The performance metrics for these two models are roughly equal, perhaps because the trend component in the <em>Random Walk with a Drift</em> was approximately flat.</p>
<p>Relatively speaking, the <em>Random Walk</em> and <em>Random Walk with a Drift</em> models outperform the others, but are they useful in forecasting returns? Actually, an RMSE of 6.5% (0.0652) is not an acceptable forecast error when considing the average monthly return of 0.9% over the nine-year period 2011-2019. It is informative to notice the MAPE (Mean Absolute Percentage Error) of 329% for these two models. Perhaps a MAPE of less than 20% would be acceptable.</p>
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>Based on the preceding analysis, it appears that stock returns cannot be effectively modeled with the handful of forecast models employed in this analysis. There appears to be too much randomness that can not be captured in the models used.</p>
</div>
<div id="other-considerations" class="section level1">
<h1>Other Considerations</h1>
<div id="type-of-analysis" class="section level3">
<h3>Type of analysis</h3>
<p>We conducted a numerical forecast of a numerical variable (monthly returns), but we could have conducted a classification forecast of a numerical variable. We could have classified monthly returns as a logical variable (positive return = TRUE, negative return = FALSE) and performed a random forest and logical regression models, among others.</p>
</div>
<div id="limitations" class="section level3">
<h3>Limitations</h3>
<p>While the results are interesting, they aren’t necessarily useful for several reasons.</p>
</div>
<div id="data-insufficiency" class="section level3">
<h3>Data Insufficiency</h3>
<p>First, there doesn’t appear to be a sufficient number of data. Ideally, we would use several decades that span economic cycles and government regimes. Moreover, the breadth of the data (we used only the S&amp;P 500 index) should include a greater range of stocks to be able to make such broad conclusions.</p>
</div>
<div id="transaction-costs" class="section level3">
<h3>Transaction Costs</h3>
<p>Another limitation is that transaction costs were not considered in the monthly return calculations which could prove costly and change the conclusions reached herein.</p>
</div>
<div id="model-insufficiency" class="section level3">
<h3>Model Insufficiency</h3>
<p>Seven forecast models were fit to the data, but there are other forecast models that could be analyzed and applied. It is possible that this could change the results and conclusions of the analysis.</p>
</div>
<div id="academic-findings" class="section level3">
<h3>Academic Findings</h3>
<p>Many leaders in the academic community have analyzed stock returns over longer time horizons and over wider groups of stocks (not just the largest 500) and found that generally, stock returns follow random walks (i.e., behave like white noise).</p>
</div>
</div>
